{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb3af54",
   "metadata": {},
   "source": [
    "<img src=\"Databuds Banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c280e",
   "metadata": {},
   "source": [
    "<font size = \"3\">\n",
    "   \n",
    " <br>**Created by**: Madeleine Sim \n",
    " <br>**Contact**: madzsyl@gmail.com\n",
    " <br>**Github**: https://github.com/madzsyl/\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57291d",
   "metadata": {},
   "source": [
    "# Who am I calling?\n",
    "\n",
    "In this hands-on session, we will be using a public dataset - Bank Marketing data to run our analysis.\n",
    "Read the instructions and fill in the answers in the accompanying form. Get all answers right and you stand a chance to win a prize (and honour and glory) in the next DataBuds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Data Source: https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9edee",
   "metadata": {},
   "source": [
    "### Step 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cab7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata = pd.read_csv(\"bankmarketing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64491e5",
   "metadata": {},
   "source": [
    "Note: If you are running this on Google colab, uncomment the lines below to load the dataset\n",
    "Load the Databuds folder in your Google drive.\n",
    "Run the following:\n",
    "\n",
    "If the path (of bank data) does not work, you can call this code:\n",
    "\n",
    "<font color ='orangered'>\n",
    "*import os</br>\n",
    "os.listdir()*</font>\n",
    "\n",
    "to locate your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389dae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')\n",
    "#bankdata = pd.read_csv(\"drive/MyDrive/DataBuds/bankmarketing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd766c7",
   "metadata": {},
   "source": [
    "## Data Set Information:\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
    "\n",
    "1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]<br>\n",
    "\n",
    "<br>\n",
    "The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).<br>\n",
    "\n",
    "\n",
    "###  Attribute Information:\n",
    "### Input variables:\n",
    "#### Bank client data:\n",
    "1 - age (numeric)<br>\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')<br>\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')<br>\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')<br>\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')<br>\n",
    "\n",
    "#### Related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')<br>\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>\n",
    "\n",
    "#### Other attributes\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)<br>\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n",
    "\n",
    "#### Social and economic context attributes:\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)<br>\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)<br>\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)<br>\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)<br>\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)<br>\n",
    "\n",
    "### Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4e04f",
   "metadata": {},
   "source": [
    "### Objective of our Clustering analysis today\n",
    "\n",
    "Before we want to identify potential customers who are likely to subscribe to a term deposit, let's start by seeing if we can come up with some customer profiles to work with. Clustering techniques are suitable for us to create customer profiles based on their data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e59157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the data after it's loaded (e.g. print the number of\n",
    "# rows and columns, print the first few rows).\n",
    "\n",
    "print(\"Shape of data:\",bankdata.shape)\n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e6af8",
   "metadata": {},
   "source": [
    "# <font color='royalblue'>Quiz 1:</font>\n",
    "\n",
    "1. Which job forms 25% of the rows\n",
    "2. How many clients have not been contacted for marketing purposes\n",
    "3. True or False - Higher proportion customers with housing loan (housing = \"yes\") has subscribed to a term deposit (y = \"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661a00a",
   "metadata": {},
   "source": [
    "Answers:</br>\n",
    "(1) </br>\n",
    "(2)</br>\n",
    "(3)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b288f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata.job.value_counts(normalize = True)\n",
    "#bankdata.loan.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.pivot_table(data = bankdata, index = \"housing\", columns = \"y\", values = \"loan\", aggfunc='count')#.reset_index()\n",
    "temp[\"total\"] = temp[\"no\"]+temp[\"yes\"]\n",
    "temp[\"no\"] = round(temp[\"no\"]/temp[\"total\"],3)\n",
    "temp[\"yes\"] = round(temp[\"yes\"]/temp[\"total\"],3)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.pivot_table(data = bankdata, index = \"loan\", columns = \"y\", values = \"housing\", aggfunc='count').reset_index()\n",
    "temp[\"total\"] = temp[\"no\"]+temp[\"yes\"]\n",
    "temp[\"no\"] = round(temp[\"no\"]/temp[\"total\"],3)\n",
    "temp[\"yes\"] = round(temp[\"yes\"]/temp[\"total\"],3)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60779d",
   "metadata": {},
   "source": [
    "# Step 1: Preprocessing\n",
    "### Question: \n",
    "How many columns are numeric and how many are categorical?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983fca3",
   "metadata": {},
   "source": [
    "Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 numeric, 11 Cat\n",
    "bankdata.info()\n",
    "\n",
    "# Note, while we wish to separate out the categorical and the numerical columns, lets do some basic conversion of columns that are simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d0f60",
   "metadata": {},
   "source": [
    "### 1.1 \n",
    "Drop industry level variables (we want to find out more about the customer. While industry level variables are also useful, we remove them for this analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed43341",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata.drop(columns = [\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"], inplace = True)\n",
    "bankdata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b018c",
   "metadata": {},
   "source": [
    "Drop Month and Day variable. We could keep them if we think that customers respond differently on different periods. However, this dataset spans 3 years - while we have the Month and Day data, we do not have data on the Year. Hence, let's drop the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata.drop(columns = [\"month\", \"day_of_week\"], inplace = True)\n",
    "bankdata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5143b",
   "metadata": {},
   "source": [
    "### 1.2 Hot encode the following Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job - Top 3 categories + Others\n",
    "def encode_var(row, col, ori_name, cat_name, others = \"Others\"):\n",
    "    t = others\n",
    "    for i in range(len(ori_name)):\n",
    "        if(row[col]==ori_name[i]):\n",
    "            t = cat_name[i]\n",
    "    return t\n",
    "\n",
    "bankdata[\"job_new\"] = bankdata.apply(lambda x: encode_var(x, \"job\", [\"admin.\", \"blue-collar\", \"services\", \"management\"], [\"Admin\", \"BC\", \"Svcs\", \"Mgmt\"] ), axis = 1)\n",
    "bankdata[[\"job\",\"job_new\"]].value_counts()\n",
    "\n",
    "bankdata[\"marital_new\"]=bankdata.apply(lambda x: encode_var(x, \"marital\", [\"married\", \"single\"], [\"married\", \"single\"] ), axis = 1)\n",
    "bankdata[[\"marital\",\"marital_new\"]].value_counts()\n",
    "\n",
    "bankdata[\"education_new\"]=bankdata.apply(lambda x: encode_var(x, \"education\", [\"university.degree\", \"professional.course\", \"high.school\"], [\"professional\", \"professional\", \"highschool\"] ), axis = 1)\n",
    "bankdata[[\"education\",\"education_new\"]].value_counts()\n",
    "\n",
    "bankdata[\"contact_tel\"]=bankdata.apply(lambda x: encode_var(x, \"contact\", [\"telephone\"], [1] , others = 0), axis = 1)\n",
    "bankdata[[\"education\",\"education_new\"]].value_counts()\n",
    "\n",
    "bankdata[\"poutcome\"] = bankdata.apply(lambda x: encode_var(x, \"poutcome\", ['failure','nonexistent','success'],[0,0,1], 0), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db796e",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Using the above example, map the variables \"default\", \"housing\", \"loan\" and \"y\" to 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61019245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "cat_yes = [\"yes\"]\n",
    "cat_1 = [1]\n",
    "\n",
    "bankdata[\"default\"] = <--- Your answer--->\n",
    "bankdata[\"housing\"] = <--- Your answer--->\n",
    "bankdata[\"loan\"] = <--- Your answer--->\n",
    "bankdata[\"y\"] = <--- Your answer--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e693356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old vars\n",
    "col_drop = [\"job\", \"marital\", \"education\", \"contact\"]\n",
    "bankdata.drop(columns = col_drop, inplace = True)\n",
    "#bankdata.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb9b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot encode categorical variables\n",
    "bankdata = pd.get_dummies(bankdata)\n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17c485",
   "metadata": {},
   "source": [
    "### Step 2: Understanding Data relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,12)})\n",
    "sns.heatmap(bankdata.corr(),cmap = \"PRGn\", annot=True, fmt = \"0.1f\")\n",
    "#bankdata.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43eb89",
   "metadata": {},
   "source": [
    "Since duration and y are reliant on each other, we remove duration now so that we do not work with data leakage if we choose to run a predictive model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = [\"duration\"]\n",
    "bankdata.drop(columns = col_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1e94a",
   "metadata": {},
   "source": [
    "### 2.1 Perform Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657af67",
   "metadata": {},
   "source": [
    "### Question for thought: Is there a need to scale our data?\n",
    "Yes and No. This depends on what you want to do later. Since we wish to run a clustering, we may likely want to perform dimensionality reduction in order to optimise the runtime (very important for large sets of data).\n",
    "\n",
    "However, dimensionality reduction will fail if the data is not scaled, as we need to pick the number of Principal Components based on the variance explained by each PC (very vulnerable to scale of data)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bankdata_fit = scaler.fit_transform(bankdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fafac5",
   "metadata": {},
   "source": [
    "### 2.2 Dimension Reduction Through PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ae2d2",
   "metadata": {},
   "source": [
    "Dimension reduction is important in cluster analysis and creates a smaller data in volume and has the same analytical results as the original representation. A clustering process needs data reduction to obtain an efficient processing time while clustering and mitigate curse of dimensionality.\n",
    "\n",
    "Since we have many variables to work with, we can look at dimensionality reduction techniques, through PCA. \n",
    "- Use sklearn's [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class to apply principal component analysis on the data, thus finding the vectors of maximal variance in the data. To start, you should not set any parameters (so all components are computed) or set a number of components that is at least half the number of features (so there's enough features to see the general trend in variability).\n",
    "- Check out the ratio of variance explained by each principal component as well as the cumulative variance explained. Try plotting the cumulative or sequential values using matplotlib's [`plot()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) function. Based on what you find, select a value for the number of transformed features you'll retain for the clustering part of the project.\n",
    "- Once you've made a choice for the number of components to keep, make sure you re-fit a PCA instance to perform the decided-on transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30488ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "prin_com = pca.fit_transform(bankdata_fit)\n",
    "pd.DataFrame(pca.explained_variance_ratio_).cumsum().head(10)\n",
    "#(pd.DataFrame(pca.explained_variance_ratio_).cumsum().plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88088178",
   "metadata": {},
   "source": [
    "# <font color='royalblue'>Quiz 2:</font>\n",
    "### If I wish to retain 65% of the data variance, how many Principal Components should I retain? \n",
    "(Hint : remember to count PC_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the value of N for which variance retained is at least 65%\n",
    "N= <--- Your answer---> \n",
    "\n",
    "pca = PCA(N)\n",
    "prin_com = pca.fit_transform(bankdata_fit)\n",
    "#pd.DataFrame(pca.explained_variance_ratio_).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192440be",
   "metadata": {},
   "source": [
    "# <font color='royalblue'>Quiz 3:</font>\n",
    "### What can I say about PC 7?\n",
    "a) It has more Admin professions<br>\n",
    "b) It has more Management professions<br>\n",
    "c) Most of them have a housing loan<br>\n",
    "\n",
    "Answer: <--- Your answer---> \n",
    "\n",
    "Note: The larger the weight of the value, the more significant the variable (sign does not matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(pca.components_, columns = bankdata.columns), annot = True,fmt = \"0.1f\", cmap = 'YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c5fc6",
   "metadata": {},
   "source": [
    "## Step 3: Clustering\n",
    "\n",
    "### Step 3.1: Apply Clustering to the full dataset\n",
    "\n",
    "You've assessed and cleaned the  data, then scaled and transformed them. Now, it's time to see how the data clusters in the principal components space. In this substep, you will apply k-means clustering to the dataset and use the average within-cluster distances from each point to their assigned cluster's centroid to decide on a number of clusters to keep.\n",
    "\n",
    "- Use sklearn's [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) class to perform k-means clustering on the PCA-transformed data.\n",
    "- Compute the score of the model **Hint**: The KMeans object's `.score()` method might be useful here.\n",
    "- Perform the above two steps for a number of different cluster counts. You can then see how the average distance decreases with an increasing number of clusters. However, each additional cluster provides a smaller net benefit. Use this fact to select a final number of clusters in which to group the data. <br>\n",
    "**Warning**: depending size of the dataset, it can take a long time for the algorithm to resolve. The more clusters to fit, the longer the algorithm will take. You should test for cluster counts through at least 10 clusters to get the full picture. Ultimately, we need to know our business needs to know how many clusters we are aiming for, and having too many clusters might hinder our understanding\n",
    "- Once you've selected a final number of clusters to use, re-fit a KMeans instance to perform the clustering operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9595187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Over a number of different cluster counts...\n",
    "scores = []\n",
    "centers = list(range(1,10,1))\n",
    "\n",
    "    # run k-means clustering on the data and...\n",
    "for c in centers:\n",
    "    #print(\"Running Model for\",c,\"clusters\")\n",
    "    model_k = KMeans(n_clusters=c)\n",
    "    model_k.fit(prin_com)\n",
    "    # compute the average within-cluster distances.\n",
    "    score = np.abs(model_k.score(prin_com))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28793821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the change in within-cluster distance across number of clusters.\n",
    "# HINT: Use matplotlib's plot function to visualize this relationship.\n",
    "plt.plot(centers, scores)\n",
    "plt.xticks(range(1,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55698156",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust = 4\n",
    "kmeans = KMeans(n_clusters = n_clust)\n",
    "kmeans.fit(prin_com)\n",
    "pred = kmeans.predict(prin_com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65827f",
   "metadata": {},
   "source": [
    "## Step 4: Making sense of your clusters\n",
    "\n",
    "### 1st way: Reversing the scaling and PC of your data to study the Centroids of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c421c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_0 = scaler.inverse_transform(pca.inverse_transform(kmeans.cluster_centers_[0]))\n",
    "over_c = pd.Series(data = centroid_0, index = bankdata.columns)\n",
    "over_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18ff4d",
   "metadata": {},
   "source": [
    "### 2nd Way: Call out the mean of a groupby(\"Pred\") object \n",
    "(a lot easier! I would prefer this because it's straightforward)<br>\n",
    "Recap: We assigned the cluster numbers to the variable \"Pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1792acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata[\"pred\"] = pred\n",
    "bankdata.groupby(\"pred\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0f536",
   "metadata": {},
   "source": [
    "# <font color='royalblue'>Quiz 4:</font>\n",
    "### Which of these statements are likely true?\n",
    "a) cluster 0 has the oldest customers</br>\n",
    "b) cluster 1 comprises the most customers who were recently contacted</br>\n",
    "c) cluster 1 has had the most defaults previously</br>\n",
    "d) cluster 1 has a higher proportion of married customers</br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f803ffa",
   "metadata": {},
   "source": [
    "# The end!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b057a1",
   "metadata": {},
   "source": [
    "This sums up the gist of how you would run an unsupervised clustering from start to end! While the steps taken can be pretty standard, it is improtant to understand your business context and data points before making decisions on how to manipulate the data!\n",
    "\n",
    "Feel free to substitute the scaling, dimensionality reduction, and clustering model with different methods to see how your data changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32397f9f",
   "metadata": {},
   "source": [
    "## What... next?\n",
    "Notice that in the dataset, a variable \"y\" exists. This variable tells us if a client subscribes to a term deposit. We could possibly build a **classification model**, if we wish to predict which customers are more likely to subscribe to a term deposit (and hence, spend our time calling them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6eb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## An Implementation of DBScan\n",
    "\n",
    "from sklearn.cluster import DBSCAN  \n",
    "\n",
    "db_cluster = DBSCAN(eps = 0.5, min_samples = 3).fit(prin_com)\n",
    "pred = db_cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "## An Implementation of Gaussian Mixed Models\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=5)\n",
    "gmm_pred = gmm.fit_predict(prin_com)\n",
    "\n",
    "## If you want the probabilities of each data point belonging to the cluster, you can do this:\n",
    "# gmm_predict_proba = gmm.fit_predict_proba(prin_com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
